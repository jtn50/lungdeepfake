{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfbe9bf4",
      "metadata": {
        "id": "dfbe9bf4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "#ML Packages\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68a1fd6e",
      "metadata": {
        "id": "68a1fd6e"
      },
      "source": [
        "Load Processed Dataset & Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "007bec43",
      "metadata": {
        "id": "007bec43"
      },
      "outputs": [],
      "source": [
        "def load_scan(path2scan):\n",
        "    if (path2scan.split('.')[-1] == 'mhd') or (path2scan.split('.')[-1] == 'raw'):\n",
        "        return load_mhd(path2scan)\n",
        "    elif path2scan.split('.')[-1] == 'dcm':\n",
        "        return load_dicom(os.path.split(path2scan)[0]) #pass containing directory\n",
        "    elif os.path.isdir(path2scan) and any(f.endswith('.dcm') for f in os.listdir(path2scan)):\n",
        "        return load_dicom(path2scan)\n",
        "    else:\n",
        "        raise Exception('No valid scan [series] found in given file/directory')\n",
        "\n",
        "def load_mhd(path2scan):\n",
        "    itkimage = sitk.ReadImage(path2scan)\n",
        "    scan = sitk.GetArrayFromImage(itkimage)\n",
        "    spacing = np.flip(np.array(itkimage.GetSpacing()),axis=0)\n",
        "    orientation = np.transpose(np.array(itkimage.GetDirection()).reshape((3, 3)))\n",
        "    origin = np.flip(np.array(itkimage.GetOrigin()),axis=0)\n",
        "    return scan, spacing, orientation, origin, None\n",
        "\n",
        "def load_dicom(path2scan_dir):\n",
        "    dicom_folder = path2scan_dir\n",
        "    dcms = [f for f in os.listdir(dicom_folder) if f.endswith('.dcm')]\n",
        "    if not dcms:\n",
        "        raise Exception(f\"No DICOM files found in {dicom_folder}\")\n",
        "\n",
        "    first_slice_data = pydicom.dcmread(os.path.join(path2scan_dir, dcms[0]))\n",
        "    first_slice = first_slice_data.pixel_array\n",
        "    orientation = np.transpose(first_slice_data.ImageOrientationPatient)\n",
        "    spacing_xy = np.array(first_slice_data.PixelSpacing, dtype=float)\n",
        "    spacing_z = float(first_slice_data.SliceThickness)\n",
        "    spacing = np.array([spacing_z, spacing_xy[1], spacing_xy[0]])\n",
        "\n",
        "    scan = np.zeros((len(dcms), first_slice.shape[0], first_slice.shape[1]))\n",
        "    raw_slices = []\n",
        "    indexes = []\n",
        "\n",
        "    for dcm in dcms:\n",
        "        slice_data = pydicom.dcmread(os.path.join(dicom_folder, dcm))\n",
        "        slice_data.filename = dcm\n",
        "        raw_slices.append(slice_data)\n",
        "        indexes.append(float(slice_data.ImagePositionPatient[2]))\n",
        "\n",
        "    indexes = np.array(indexes, dtype=float)\n",
        "    raw_slices = [x for _, x in sorted(zip(indexes, raw_slices))]\n",
        "\n",
        "    try:\n",
        "        origin = np.array(raw_slices[0][0x00200032].value)\n",
        "        origin = np.array([origin[2], origin[1], origin[0]])\n",
        "    except:\n",
        "        origin = np.zeros(3)\n",
        "\n",
        "    for i, slice_obj in enumerate(raw_slices):\n",
        "        scan[i, :, :] = slice_obj.pixel_array\n",
        "\n",
        "    return scan, spacing, orientation, origin, raw_slices\n",
        "\n",
        "def scale_scan(scan, spacing, factor=1):\n",
        "    resize_factor = factor * spacing\n",
        "    new_real_shape = scan.shape * resize_factor\n",
        "    new_shape = np.round(new_real_shape)\n",
        "    real_resize_factor = new_shape / scan.shape\n",
        "    new_spacing = spacing / real_resize_factor\n",
        "    scan_resized = scipy.ndimage.zoom(scan, real_resize_factor, mode='nearest')\n",
        "    return scan_resized, resize_factor\n",
        "\n",
        "def get_scaled_shape(shape, spacing):\n",
        "    new_real_shape = shape * spacing\n",
        "    return np.round(new_real_shape).astype(int)\n",
        "\n",
        "def world2vox(world_coord, spacing, orientation, origin):\n",
        "    world_coord = np.dot(np.linalg.inv(np.dot(orientation, np.diag(spacing))), world_coord - origin)\n",
        "    if orientation[0, 0] < 0:\n",
        "        vox_coord = (np.array([world_coord[0], world_coord[2], world_coord[1]])).astype(int)\n",
        "    else:\n",
        "        vox_coord = (np.array([world_coord[0], world_coord[1], world_coord[2]])).astype(int)\n",
        "    return vox_coord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c313462",
      "metadata": {
        "id": "5c313462"
      },
      "outputs": [],
      "source": [
        "class LabeledExtractor:\n",
        "    \"\"\"\n",
        "    Adapted extractor for labeled medical imaging dataset to meet new pydicom requirements\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 labels_csv_path,\n",
        "                 dicom_base_dir,\n",
        "                 dst_path,\n",
        "                 norm_save_dir,\n",
        "                 cube_shape=(32, 32, 32),\n",
        "                 parallelize=False,\n",
        "                 coordSystem='vox',\n",
        "                 include_types=['FB', 'FM', 'TB', 'TM'],\n",
        "                 augment=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            labels_csv_path: Path to labels file\n",
        "            dicom_base_dir: Base directory containing DICOM scans organized by uuid\n",
        "            dst_path: Where to save the processed dataset\n",
        "            norm_save_dir: Where to save normalization parameters\n",
        "            cube_shape: Size of extracted 3D cubes\n",
        "            parallelize: Whether to use multiprocessing\n",
        "            coordSystem: 'vox' for voxel coordinates\n",
        "            include_types: Which finding types to include\n",
        "            augment: Whether to perform data augmentation\n",
        "        \"\"\"\n",
        "        self.labels_csv_path = labels_csv_path\n",
        "        self.dicom_base_dir = dicom_base_dir\n",
        "        self.dst_path = dst_path\n",
        "        self.norm_save_dir = norm_save_dir\n",
        "        self.cube_shape = cube_shape\n",
        "        self.parallelize = parallelize\n",
        "        self.coordSystem = coordSystem\n",
        "        self.include_types = include_types\n",
        "        self.augment = augment\n",
        "\n",
        "        # Load and filter the labels\n",
        "        self.labels_df = pd.read_csv(labels_csv_path)\n",
        "        self.labels_df = self.labels_df[self.labels_df['type'].isin(include_types)]\n",
        "\n",
        "        # # Filter out TB entries with 0,0,0 coordinates (no specific location)\n",
        "        # self.labels_df = self.labels_df[~((self.labels_df['type'] == 'TB') &\n",
        "        #                                 (self.labels_df['x'] == 0) &\n",
        "        #                                 (self.labels_df['y'] == 0) &\n",
        "        #                                 (self.labels_df['slice'] == 0))]\n",
        "\n",
        "        print(f\"Loaded {len(self.labels_df)} labeled samples\")\n",
        "        print(f\"Types distribution:\\n{self.labels_df['type'].value_counts()}\")\n",
        "\n",
        "    def extract(self, plot=True):\n",
        "        \"\"\"Extract and process all labeled samples\"\"\"\n",
        "        print(\"Preparing extraction jobs...\")\n",
        "\n",
        "        jobs = []\n",
        "        labels = []\n",
        "\n",
        "        for idx, row in self.labels_df.iterrows():\n",
        "            # Build path to DICOM scan\n",
        "            scan_path = os.path.join(self.dicom_base_dir, str(row['uuid']))\n",
        "\n",
        "            # Coordinate in z,y,x format (slice, y, x)\n",
        "            coord = np.array([row['slice'], row['y'], row['x']])\n",
        "\n",
        "            # Job format: [scan_path, coord, cube_shape, coordSystem, label_type]\n",
        "            jobs.append([scan_path, coord, self.cube_shape, self.coordSystem, row['type']])\n",
        "            labels.append(row['type'])\n",
        "\n",
        "        print(f\"Extracting {len(jobs)} samples...\")\n",
        "\n",
        "        if self.parallelize:\n",
        "            num_cores = int(np.ceil(min(np.ceil(multiprocessing.cpu_count() * 0.75), len(jobs))))\n",
        "            results = Parallel(n_jobs=num_cores)(delayed(self._process_job)(job) for job in jobs)\n",
        "        else:\n",
        "            results = []\n",
        "            for i, job in enumerate(jobs):\n",
        "                try:\n",
        "                    result = self._process_job(job)\n",
        "                    results.append(result)\n",
        "                    if i % 10 == 0:\n",
        "                        print(f\"Processed {i+1}/{len(jobs)} samples\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to process sample {job[0]}: {e}\")\n",
        "                    results.append(None)\n",
        "\n",
        "        # Collect successful extractions\n",
        "        instances = []\n",
        "        instance_labels = []\n",
        "\n",
        "        for i, result in enumerate(results):\n",
        "            if result is not None:\n",
        "                cubes, label_type = result\n",
        "                instances.extend(cubes)\n",
        "                instance_labels.extend([label_type] * len(cubes))\n",
        "\n",
        "        instances = np.array(instances)\n",
        "        instance_labels = np.array(instance_labels)\n",
        "\n",
        "        if len(instances) == 0:\n",
        "            print(\"ERROR: No instances were successfully extracted!\")\n",
        "            print(\"This could be due to:\")\n",
        "            print(\"1. DICOM loading issues\")\n",
        "            print(\"2. Coordinate problems\")\n",
        "            print(\"3. Missing DICOM files\")\n",
        "            return None, None, None\n",
        "\n",
        "        print(f\"Successfully extracted {len(instances)} instances\")\n",
        "\n",
        "        # Preprocessing\n",
        "        print(\"Equalizing the data...\")\n",
        "        eq = histEq(instances)\n",
        "        instances = eq.equalize(instances)\n",
        "        os.makedirs(self.norm_save_dir, exist_ok=True)\n",
        "        eq.save(path=os.path.join(self.norm_save_dir, 'equalization.pkl'))\n",
        "\n",
        "        print(\"Normalizing the data...\")\n",
        "        min_v = np.min(instances)\n",
        "        max_v = np.max(instances)\n",
        "        mean_v = np.mean(instances)\n",
        "        norm_data = np.array([mean_v, min_v, max_v])\n",
        "        instances = (instances - mean_v) / (max_v - min_v)\n",
        "        np.save(os.path.join(self.norm_save_dir, 'normalization.npy'), norm_data)\n",
        "\n",
        "        if plot:\n",
        "            self._plot_samples(instances, instance_labels)\n",
        "\n",
        "        # Save dataset and labels\n",
        "        print(\"Saving the dataset...\")\n",
        "        np.save(self.dst_path, instances)\n",
        "        np.save(self.dst_path.replace('.npy', '_labels.npy'), instance_labels)\n",
        "\n",
        "        # Create label mapping\n",
        "        label_mapping = {label: idx for idx, label in enumerate(np.unique(instance_labels))}\n",
        "        print(f\"Label mapping: {label_mapping}\")\n",
        "\n",
        "        return instances, instance_labels, label_mapping\n",
        "\n",
        "    def _process_job(self, args):\n",
        "        \"\"\"Process single extraction job\"\"\"\n",
        "        scan_path, coord, cube_shape, coordSystem, label_type = args\n",
        "\n",
        "        try:\n",
        "            # Load scan\n",
        "            scan, spacing, orientation, origin, raw_slices = load_scan(scan_path)\n",
        "\n",
        "            # Convert coordinates if needed\n",
        "            if coordSystem == 'world':\n",
        "                coord = world2vox(coord, spacing, orientation, origin)\n",
        "\n",
        "            # Extract the base cube with padding\n",
        "            init_cube_shape = get_scaled_shape(np.array(cube_shape) + 8, 1/spacing)\n",
        "            clean_cube_unscaled = cutCube(scan, coord, init_cube_shape, padd=-1000)\n",
        "\n",
        "            # Scale the cube\n",
        "            scaled_cube, resize_factor = scale_scan(clean_cube_unscaled, spacing)\n",
        "\n",
        "            # Data augmentation\n",
        "            if self.augment:\n",
        "                augmented_cubes = self._augment_instance(scaled_cube)\n",
        "            else:\n",
        "                augmented_cubes = [scaled_cube]\n",
        "\n",
        "            # Trim to final shape\n",
        "            final_cubes = []\n",
        "            for cube in augmented_cubes:\n",
        "                center = np.array(cube.shape) // 2\n",
        "                final_cube = cutCube(cube, center, cube_shape, padd=-1000)\n",
        "                if final_cube.shape == tuple(cube_shape):  # Only keep properly shaped cubes\n",
        "                    final_cubes.append(final_cube)\n",
        "\n",
        "            return final_cubes, label_type\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {scan_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _augment_instance(self, x0):\n",
        "        \"\"\"Data augmentation similar to original code but simplified\"\"\"\n",
        "        augmented = [x0]  # Original\n",
        "\n",
        "        # Flips\n",
        "        augmented.append(np.flip(x0, 1))  # x flip\n",
        "        augmented.append(np.flip(x0, 2))  # y flip\n",
        "\n",
        "        # Small shifts\n",
        "        augmented.append(scipy.ndimage.shift(x0, (0, 2, 2), mode='constant'))\n",
        "        augmented.append(scipy.ndimage.shift(x0, (0, -2, 2), mode='constant'))\n",
        "\n",
        "        # Small rotations\n",
        "        for angle in [15, 30, 45, 90, 180]:\n",
        "            rotated = rotate(x0, angle, axes=(1, 2), mode='reflect', reshape=False)\n",
        "            augmented.append(rotated)\n",
        "\n",
        "        # Filter out invalid shapes\n",
        "        valid_cubes = []\n",
        "        for cube in augmented:\n",
        "            if cube.shape[0] > 0 and cube.shape[1] > 0 and cube.shape[2] > 0:\n",
        "                valid_cubes.append(cube)\n",
        "\n",
        "        return valid_cubes\n",
        "\n",
        "    def _plot_samples(self, instances, labels):\n",
        "        \"\"\"Plot sample instances with their labels\"\"\"\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # Select random samples from each class\n",
        "        unique_labels = np.unique(labels)\n",
        "        samples_per_class = min(5, len(instances) // len(unique_labels))\n",
        "\n",
        "        fig, axes = plt.subplots(len(unique_labels), samples_per_class,\n",
        "                                figsize=(samples_per_class * 3, len(unique_labels) * 3))\n",
        "\n",
        "        if len(unique_labels) == 1:\n",
        "            axes = axes.reshape(1, -1)\n",
        "\n",
        "        for i, label in enumerate(unique_labels):\n",
        "            label_indices = np.where(labels == label)[0]\n",
        "            selected_indices = np.random.choice(label_indices,\n",
        "                                              min(samples_per_class, len(label_indices)),\n",
        "                                              replace=False)\n",
        "\n",
        "            for j, idx in enumerate(selected_indices):\n",
        "                middle_slice = instances[idx].shape[0] // 2\n",
        "                axes[i, j].imshow(instances[idx][middle_slice, :, :], cmap='bone')\n",
        "                axes[i, j].set_title(f'{label}')\n",
        "                axes[i, j].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.suptitle('Sample Extracted Instances by Type')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4bbb818",
      "metadata": {
        "id": "a4bbb818"
      },
      "outputs": [],
      "source": [
        "class LabeledDataLoader:\n",
        "    \"\"\"\n",
        "    Data loader for labeled dataset - modified for classification\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset_path, labels_path, normdata_path, img_res=(32, 32, 32)):\n",
        "        self.normdata_path = normdata_path\n",
        "        self.img_res = img_res\n",
        "\n",
        "        print(\"Loading preprocessed dataset...\")\n",
        "        self.data = np.load(dataset_path)\n",
        "        self.labels = np.load(labels_path)\n",
        "\n",
        "        # Create label mapping\n",
        "        self.unique_labels = np.unique(self.labels)\n",
        "        self.label_to_idx = {label: idx for idx, label in enumerate(self.unique_labels)}\n",
        "        self.idx_to_label = {idx: label for label, idx in self.label_to_idx.items()}\n",
        "\n",
        "        # Convert labels to indices\n",
        "        self.label_indices = np.array([self.label_to_idx[label] for label in self.labels])\n",
        "\n",
        "        # Format for neural network\n",
        "        self.data = self.data.reshape((len(self.data), self.img_res[0], self.img_res[1], self.img_res[2], 1))\n",
        "\n",
        "        print(f\"Loaded {len(self.data)} samples\")\n",
        "        print(f\"Label distribution: {dict(zip(*np.unique(self.labels, return_counts=True)))}\")\n",
        "\n",
        "    def load_batch(self, batch_size=32, shuffle=True):\n",
        "        \"\"\"Load batches for training\"\"\"\n",
        "        if shuffle:\n",
        "            indices = np.random.permutation(len(self.data))\n",
        "        else:\n",
        "            indices = np.arange(len(self.data))\n",
        "\n",
        "        n_batches = len(self.data) // batch_size\n",
        "\n",
        "        for i in range(n_batches):\n",
        "            batch_indices = indices[i * batch_size:(i + 1) * batch_size]\n",
        "            batch_data = self.data[batch_indices]\n",
        "            batch_labels = self.label_indices[batch_indices]\n",
        "\n",
        "            yield batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "626f555c",
      "metadata": {
        "id": "626f555c"
      },
      "outputs": [],
      "source": [
        "def create_eda_dataframe(labels_csv_path, processed_data_path=None, processed_labels_path=None):\n",
        "    \"\"\"\n",
        "    Create dataframe for EDA\n",
        "    \"\"\"\n",
        "    # Load original labels CSV\n",
        "    print(\"Loading original labels...\")\n",
        "    df_filtered = pd.read_csv(labels_csv_path)\n",
        "\n",
        "    # # Filter out TB entries with 0,0,0 coordinates\n",
        "    # df_filtered = df_original[df_original['type'].isin(['FB', 'FM', 'TM'])]\n",
        "    # df_filtered = df_filtered[~((df_filtered['type'] == 'TB') &\n",
        "    #                           (df_filtered['x'] == 0) &\n",
        "    #                           (df_filtered['y'] == 0) &\n",
        "    #                           (df_filtered['slice'] == 0))]\n",
        "\n",
        "    #print(f\"Original labels: {len(df_original)}\")\n",
        "    print(f\"After filtering: {len(df_filtered)}\")\n",
        "\n",
        "    # Add encoded labels for visualization\n",
        "    le = LabelEncoder()\n",
        "    df_filtered['type_encoded'] = le.fit_transform(df_filtered['type'])\n",
        "\n",
        "    # add augmentation info\n",
        "    if processed_data_path and processed_labels_path:\n",
        "        try:\n",
        "            processed_labels = np.load(processed_labels_path)\n",
        "            print(f\"Processed dataset size: {len(processed_labels)}\")\n",
        "\n",
        "            # Calculate augmentation factor\n",
        "            augmentation_factor = len(processed_labels) / len(df_filtered)\n",
        "            df_filtered['augmentation_factor'] = augmentation_factor\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(\"Processed data not found, using original data only\")\n",
        "            df_filtered['augmentation_factor'] = 1\n",
        "\n",
        "    return df_filtered, le\n",
        "\n",
        "def plot_class_distribution(df):\n",
        "    \"\"\"Create class distribution bar plot\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Count by class\n",
        "    class_counts = df['type'].value_counts()\n",
        "    classes = class_counts.index\n",
        "    counts = class_counts.values\n",
        "\n",
        "    # Color scheme\n",
        "    colors = {'FB': '#FF6B6B', 'FM': '#4ECDC4', 'TM': '#45B7D1'}\n",
        "    plot_colors = [colors[cls] for cls in classes]\n",
        "\n",
        "    # Create bar plot\n",
        "    bars = plt.bar(classes, counts, color=plot_colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "    # Styling\n",
        "    plt.title('Distribution of Tamper Types in Dataset', fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.xlabel('Tamper Classification', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Number of Instances', fontsize=12, fontweight='bold')\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, count in zip(bars, counts):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "                str(count), ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
        "\n",
        "    # Add percentage labels\n",
        "    total = sum(counts)\n",
        "    for bar, count in zip(bars, counts):\n",
        "        percentage = (count/total) * 100\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()/2,\n",
        "                f'{percentage:.1f}%', ha='center', va='center',\n",
        "                fontweight='bold', fontsize=10, color='white')\n",
        "\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print statistics\n",
        "    print(\"\\\\n=== Class Distribution Statistics ===\")\n",
        "    for cls in classes:\n",
        "        count = class_counts[cls]\n",
        "        pct = (count/total) * 100\n",
        "        print(f\"{cls}: {count} instances ({pct:.1f}%)\")\n",
        "\n",
        "def plot_spatial_distribution(df):\n",
        "    \"\"\"Create 3D spatial distribution visualization\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    # Color mapping for tamper types\n",
        "    colors = {'FB': 0, 'FM': 1, 'TM': 2}\n",
        "    df['color_code'] = df['type'].map(colors)\n",
        "\n",
        "    # 1. X-Y plane distribution (axial view)\n",
        "    scatter1 = axes[0,0].scatter(df['x'], df['y'], c=df['color_code'],\n",
        "                                cmap='viridis', alpha=0.7, s=50, edgecolors='black')\n",
        "    axes[0,0].set_xlabel('X Coordinate (Left-Right)', fontsize=10, fontweight='bold')\n",
        "    axes[0,0].set_ylabel('Y Coordinate (Anterior-Posterior)', fontsize=10, fontweight='bold')\n",
        "    axes[0,0].set_title('Tamper Locations in X-Y Plane (Axial View)', fontsize=12, fontweight='bold')\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. X-Z plane distribution (coronal view)\n",
        "    scatter2 = axes[0,1].scatter(df['x'], df['slice'], c=df['color_code'],\n",
        "                                cmap='viridis', alpha=0.7, s=50, edgecolors='black')\n",
        "    axes[0,1].set_xlabel('X Coordinate (Left-Right)', fontsize=10, fontweight='bold')\n",
        "    axes[0,1].set_ylabel('Z Coordinate (Slice Number)', fontsize=10, fontweight='bold')\n",
        "    axes[0,1].set_title('Tamper Locations in X-Z Plane (Coronal View)', fontsize=12, fontweight='bold')\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Y-Z plane distribution (sagittal view)\n",
        "    scatter3 = axes[1,0].scatter(df['y'], df['slice'], c=df['color_code'],\n",
        "                                cmap='viridis', alpha=0.7, s=50, edgecolors='black')\n",
        "    axes[1,0].set_xlabel('Y Coordinate (Anterior-Posterior)', fontsize=10, fontweight='bold')\n",
        "    axes[1,0].set_ylabel('Z Coordinate (Slice Number)', fontsize=10, fontweight='bold')\n",
        "    axes[1,0].set_title('Tamper Locations in Y-Z Plane (Sagittal View)', fontsize=12, fontweight='bold')\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. 3D histogram\n",
        "    axes[1,1].hist2d(df['x'], df['y'], bins=20, cmap='Blues', alpha=0.8)\n",
        "    axes[1,1].set_xlabel('X Coordinate', fontsize=10, fontweight='bold')\n",
        "    axes[1,1].set_ylabel('Y Coordinate', fontsize=10, fontweight='bold')\n",
        "    axes[1,1].set_title('Spatial Density Heatmap', fontsize=12, fontweight='bold')\n",
        "\n",
        "    # Add colorbar for tamper types\n",
        "    cbar = plt.colorbar(scatter1, ax=axes[0,0], ticks=[0, 1, 2])\n",
        "    cbar.set_ticklabels(['FB', 'FM', 'TM'])\n",
        "    cbar.set_label('Tamper Type', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print spatial statistics\n",
        "    print(\"\\\\n=== Spatial Distribution Statistics ===\")\n",
        "    for coord in ['x', 'y', 'slice']:\n",
        "        print(f\"{coord.upper()} coordinate:\")\n",
        "        print(f\"  Range: [{df[coord].min()}, {df[coord].max()}]\")\n",
        "        print(f\"  Mean: {df[coord].mean():.1f} ± {df[coord].std():.1f}\")\n",
        "        print(f\"  Median: {df[coord].median():.1f}\")\n",
        "\n",
        "def plot_correlation_analysis(df):\n",
        "    \"\"\"Create correlation analysis visualization\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # 1. Correlation heatmap\n",
        "    coords = ['x', 'y', 'slice']\n",
        "    corr_matrix = df[coords].corr()\n",
        "\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "                square=True, ax=axes[0], cbar_kws={'label': 'Correlation Coefficient'})\n",
        "    axes[0].set_title('Spatial Coordinate Correlations', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # 2. Pairwise relationships\n",
        "    # Create subplot for pairwise scatter\n",
        "    axes[1].scatter(df['x'], df['y'], alpha=0.6, s=30, label='X vs Y')\n",
        "    axes[1].set_xlabel('X Coordinate', fontweight='bold')\n",
        "    axes[1].set_ylabel('Y Coordinate', fontweight='bold')\n",
        "    axes[1].set_title('X-Y Coordinate Relationship', fontsize=14, fontweight='bold')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Add trend line\n",
        "    z = np.polyfit(df['x'], df['y'], 1)\n",
        "    p = np.poly1d(z)\n",
        "    axes[1].plot(df['x'], p(df['x']), \"r--\", alpha=0.8,\n",
        "                label=f'Trend (r={corr_matrix.loc[\"x\", \"y\"]:.3f})')\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print correlation insights\n",
        "    print(\"\\\\n=== Correlation Analysis ===\")\n",
        "    print(\"Correlation coefficients:\")\n",
        "    for i in range(len(coords)):\n",
        "        for j in range(i+1, len(coords)):\n",
        "            corr_val = corr_matrix.iloc[i, j]\n",
        "            coord1, coord2 = coords[i], coords[j]\n",
        "            strength = \"weak\" if abs(corr_val) < 0.3 else \"moderate\" if abs(corr_val) < 0.7 else \"strong\"\n",
        "            print(f\"  {coord1.upper()}-{coord2.upper()}: {corr_val:.3f} ({strength})\")\n",
        "\n",
        "def plot_class_characteristics(df):\n",
        "    \"\"\"Analyze characteristics by tamper type\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Box plots for each coordinate by class\n",
        "    coords = ['x', 'y', 'slice']\n",
        "    for i, coord in enumerate(coords):\n",
        "        if i < 3:  # We have 3 coordinates but 4 subplots\n",
        "            row, col = divmod(i, 2)\n",
        "            data_by_class = [df[df['type'] == cls][coord].values for cls in ['FB', 'FM', 'TM']]\n",
        "\n",
        "            bp = axes[row, col].boxplot(data_by_class, labels=['FB', 'FM', 'TM'],\n",
        "                                       patch_artist=True, notch=True)\n",
        "\n",
        "            # Color the boxes\n",
        "            colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
        "            for patch, color in zip(bp['boxes'], colors):\n",
        "                patch.set_facecolor(color)\n",
        "                patch.set_alpha(0.7)\n",
        "\n",
        "            axes[row, col].set_title(f'{coord.upper()} Coordinate by Tamper Type',\n",
        "                                   fontweight='bold', fontsize=12)\n",
        "            axes[row, col].set_ylabel(f'{coord.upper()} Value', fontweight='bold')\n",
        "            axes[row, col].grid(True, alpha=0.3)\n",
        "\n",
        "    # Patient distribution (UUID analysis)\n",
        "    uuid_counts = df['uuid'].value_counts()\n",
        "    axes[1, 1].hist(uuid_counts.values, bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    axes[1, 1].set_title('Tamper Instances per Patient', fontweight='bold', fontsize=12)\n",
        "    axes[1, 1].set_xlabel('Number of Tamper Sites per Patient', fontweight='bold')\n",
        "    axes[1, 1].set_ylabel('Number of Patients', fontweight='bold')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print class statistics\n",
        "    print(\"\\\\n=== Class-Specific Statistics ===\")\n",
        "    for tamper_type in ['FB', 'FM', 'TB', 'TM']:\n",
        "        subset = df[df['type'] == tamper_type]\n",
        "        print(f\"\\\\n{tamper_type} ({len(subset)} instances):\")\n",
        "        print(f\"  Unique patients: {subset['uuid'].nunique()}\")\n",
        "        print(f\"  Avg instances per patient: {len(subset)/subset['uuid'].nunique():.2f}\")\n",
        "        for coord in ['x', 'y', 'slice']:\n",
        "            print(f\"  {coord.upper()}: {subset[coord].mean():.1f} ± {subset[coord].std():.1f}\")\n",
        "\n",
        "def create_augmentation_visualization(original_data_path, augmented_data_path, labels_path):\n",
        "    \"\"\"Show before/after augmentation if data available\"\"\"\n",
        "    try:\n",
        "        original_data = np.load(original_data_path) if original_data_path else None\n",
        "        augmented_data = np.load(augmented_data_path)\n",
        "        labels = np.load(labels_path)\n",
        "\n",
        "        # Sample one from each class\n",
        "        fig, axes = plt.subplots(3, 6, figsize=(18, 9))\n",
        "\n",
        "        unique_labels = np.unique(labels)\n",
        "        for class_idx, label in enumerate(unique_labels):\n",
        "            class_indices = np.where(labels == label)[0]\n",
        "            sample_idx = class_indices[0]  # First sample of this class\n",
        "\n",
        "            # Show original\n",
        "            sample_cube = augmented_data[sample_idx]\n",
        "            middle_slice = sample_cube.shape[0] // 2\n",
        "\n",
        "            axes[class_idx, 0].imshow(sample_cube[middle_slice, :, :], cmap='bone')\n",
        "            axes[class_idx, 0].set_title(f'{label} - Original')\n",
        "            axes[class_idx, 0].axis('off')\n",
        "\n",
        "            # Show some augmented versions\n",
        "            augmentation_names = ['Example 2', 'Example 3', 'Example 4', 'Example 5', 'Example 6']\n",
        "            for aug_idx in range(1, 6):\n",
        "                if sample_idx + aug_idx < len(class_indices):\n",
        "                    aug_sample_idx = class_indices[sample_idx + aug_idx] if sample_idx + aug_idx < len(class_indices) else class_indices[-1]\n",
        "                    aug_cube = augmented_data[aug_sample_idx]\n",
        "                    axes[class_idx, aug_idx].imshow(aug_cube[middle_slice, :, :], cmap='bone')\n",
        "                    axes[class_idx, aug_idx].set_title(f'{label} - {augmentation_names[aug_idx-1]}')\n",
        "                else:\n",
        "                    axes[class_idx, aug_idx].text(0.5, 0.5, 'N/A', ha='center', va='center', transform=axes[class_idx, aug_idx].transAxes)\n",
        "                axes[class_idx, aug_idx].axis('off')\n",
        "\n",
        "        plt.suptitle('Sample Images by Tamper Type', fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"\\\\n=== Augmentation Statistics ===\")\n",
        "        print(f\"Final dataset size: {len(augmented_data)} cubes\")\n",
        "        print(f\"Augmentation factor: {len(augmented_data) / len(np.unique(labels))} per unique label\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Augmented data files not found. Skipping augmentation visualization.\")\n",
        "\n",
        "def run_complete_eda(labels_csv_path, processed_data_path=None, processed_labels_path=None):\n",
        "    \"\"\"\n",
        "    Run complete EDA pipeline\n",
        "    \"\"\"\n",
        "    print(\"=== MEDICAL IMAGE TAMPER DETECTION - EDA ===\\\\n\")\n",
        "\n",
        "    # Create dataframe\n",
        "    df, label_encoder = create_eda_dataframe(labels_csv_path, processed_data_path, processed_labels_path)\n",
        "\n",
        "    print(f\"\\\\nDataset loaded successfully!\")\n",
        "    print(f\"Total instances: {len(df)}\")\n",
        "    print(f\"Unique patients: {df['uuid'].nunique()}\")\n",
        "    print(f\"Tamper types: {df['type'].unique()}\")\n",
        "\n",
        "    # Run all visualizations\n",
        "    print(\"\\\\n\" + \"=\"*50)\n",
        "    print(\"1. CLASS DISTRIBUTION ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "    plot_class_distribution(df)\n",
        "\n",
        "    print(\"\\\\n\" + \"=\"*50)\n",
        "    print(\"2. SPATIAL DISTRIBUTION ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "    plot_spatial_distribution(df)\n",
        "\n",
        "    print(\"\\\\n\" + \"=\"*50)\n",
        "    print(\"3. CORRELATION ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "    plot_correlation_analysis(df)\n",
        "\n",
        "    print(\"\\\\n\" + \"=\"*50)\n",
        "    print(\"4. CLASS CHARACTERISTICS\")\n",
        "    print(\"=\"*50)\n",
        "    plot_class_characteristics(df)\n",
        "\n",
        "    # If processed data available, show augmentation examples\n",
        "    if processed_data_path and processed_labels_path:\n",
        "        print(\"\\\\n\" + \"=\"*50)\n",
        "        print(\"5. AUGMENTATION EXAMPLES\")\n",
        "        print(\"=\"*50)\n",
        "        create_augmentation_visualization(None, processed_data_path, processed_labels_path)\n",
        "\n",
        "    return df, label_encoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69e1c2e4",
      "metadata": {
        "id": "69e1c2e4"
      },
      "outputs": [],
      "source": [
        "#Original Labels\n",
        "#labels_csv = \"/mnt/c/Users/nhanj/Desktop/deepfakes+medical+image+tamper+detection/data/Tampered Scans/labels_exp1.csv\"\n",
        "labels_csv = \"labels_exp1_imputed.csv\"\n",
        "#Augmented & TB Class-Imputed Labels and Data\n",
        "processed_data = \"combined_tampered_scans_dataset.npy\"\n",
        "processed_labels = \"combined_tampered_scans_dataset_labels.npy\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5096f07",
      "metadata": {
        "id": "c5096f07"
      },
      "outputs": [],
      "source": [
        "# image_data = np.load(processed_data)\n",
        "# #Reshape to (# Samples, 32, 32, 32, 1) since we are dealing with 32x32x32 cubes we've extracted with 1 color channel (grayscale)\n",
        "# img_res=(32, 32, 32)\n",
        "# image_data = image_data.reshape((len(image_data), img_res[0], img_res[1], img_res[2], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e25b9d88",
      "metadata": {
        "id": "e25b9d88"
      },
      "outputs": [],
      "source": [
        "# print(image_data.shape)\n",
        "# #Looking at the first cube (have to remove color channel dimension to simplify it)\n",
        "# first_cube = image_data[0, :, :, :, 0]\n",
        "# print('First pixel:', first_cube[0][0][0])\n",
        "# print('Middle pixel:', first_cube[15][15][15]) #32/16 - 1\n",
        "# print('Last pixel:', first_cube[31][31][31]) #32 - 1 due to index starting at 0\n",
        "# print(f\"Min: {image_data.min()}\") #To check that pixels are normalized to be range -0.5 to 0.5\n",
        "# print(f\"Max: {image_data.max()}\")\n",
        "# print(f\"Mean: {image_data.mean()}\")\n",
        "# print(f\"Std: {image_data.std()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5de0cf16",
      "metadata": {
        "id": "5de0cf16"
      },
      "outputs": [],
      "source": [
        "# image_labels = np.load(processed_labels)\n",
        "# print(\"Labels Shape:\", image_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95288cdf",
      "metadata": {
        "id": "95288cdf"
      },
      "source": [
        "Create Train, Validation, Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a0821c4",
      "metadata": {
        "id": "1a0821c4"
      },
      "outputs": [],
      "source": [
        "#Random Shuffle\n",
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "\n",
        "# data_length = len(image_data)\n",
        "# indices = np.arange(data_length)\n",
        "# np.random.shuffle(indices)\n",
        "# #Shuffled Datasets\n",
        "# image_data = image_data[indices]\n",
        "# image_labels = image_labels[indices]\n",
        "\n",
        "# #Data Splits\n",
        "# splits = (0.6, 0.2, 0.2) #60 train / 20 val / 20 test\n",
        "# #Train Range\n",
        "# train_end = int(splits[0] * data_length)\n",
        "# #Val Range\n",
        "# val_end = train_end + int(splits[1] * data_length)\n",
        "\n",
        "# #Train Set\n",
        "# X_train = image_data[:train_end]\n",
        "# y_train = image_labels[:train_end]\n",
        "\n",
        "# #Val Set\n",
        "# X_val = image_data[train_end:val_end]\n",
        "# y_val = image_labels[train_end:val_end]\n",
        "\n",
        "# #Test Set\n",
        "# X_test = image_data[val_end:]\n",
        "# y_test = image_labels[val_end:]\n",
        "\n",
        "# print(f\"X_train shape {X_train.shape}\")\n",
        "# print(f\"y_train shape {y_train.shape}\")\n",
        "# print(f\"X_val shape {X_val.shape}\")\n",
        "# print(f\"y_val shape {y_val.shape}\")\n",
        "# print(f\"X_test shape {X_test.shape}\")\n",
        "# print(f\"y_test shape {y_test.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27fa3a2e",
      "metadata": {
        "id": "27fa3a2e"
      },
      "source": [
        "(Sparse / Integer) Encoding Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7901bfed",
      "metadata": {
        "id": "7901bfed"
      },
      "outputs": [],
      "source": [
        "# print(\"Before:\", y_train[0])\n",
        "\n",
        "# le = LabelEncoder()\n",
        "# #Fit on training\n",
        "# y_train_encoded = le.fit_transform(y_train)\n",
        "# #Transform based on Training fit\n",
        "# y_val_encoded = le.transform(y_val)\n",
        "# #Transform based on Training fit\n",
        "# y_test_encoded = le.transform(y_test)\n",
        "\n",
        "# print(\"After:\", y_train_encoded[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AbHP7tVr0GSd",
      "metadata": {
        "id": "AbHP7tVr0GSd"
      },
      "outputs": [],
      "source": [
        "processed_training_data = \"combined_tampered_scans_dataset_train.npy\"\n",
        "processed_training_labels = \"combined_tampered_scans_dataset_train_labels.npy\"\n",
        "\n",
        "processed_val_data = \"combined_tampered_scans_dataset_val.npy\"\n",
        "processed_val_labels = \"combined_tampered_scans_dataset_val_labels.npy\"\n",
        "\n",
        "processed_test_data = \"combined_tampered_scans_dataset_test.npy\"\n",
        "processed_test_labels = \"combined_tampered_scans_dataset_test_labels.npy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nMqKKleMw5mM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMqKKleMw5mM",
        "outputId": "a2d26216-48ce-42bc-c797-01799cb8e027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(970, 32, 32, 32, 1) (33, 32, 32, 32, 1) (33, 32, 32, 32, 1)\n",
            "Shape of training data: (970, 32, 32, 32, 1), Shape of training labels: (970,)\n",
            "Shape of validation data: (33, 32, 32, 32, 1), Shape of training labels: (33,)\n",
            "Shape of training data: (33, 32, 32, 32, 1), Shape of training labels: (33,)\n"
          ]
        }
      ],
      "source": [
        "#Using the newly processed dataset with the TB class via Vishnu's randomization technique\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Loading Training data\n",
        "loaded_train_data = np.load(processed_training_data, allow_pickle=True)\n",
        "loaded_train_labels = np.load(processed_training_labels, allow_pickle=True)\n",
        "\n",
        "# Loading Validation data\n",
        "loaded_val_data = np.load(processed_val_data, allow_pickle=True)\n",
        "loaded_val_labels = np.load(processed_val_labels, allow_pickle=True)\n",
        "\n",
        "# Loading Test data\n",
        "loaded_test_data = np.load(processed_test_data, allow_pickle=True)\n",
        "loaded_test_labels = np.load(processed_test_labels, allow_pickle=True)\n",
        "\n",
        "img_res=(32, 32, 32)\n",
        "train_image_data = loaded_train_data.reshape((len(loaded_train_data), img_res[0], img_res[1], img_res[2], 1))\n",
        "val_image_data = loaded_val_data.reshape((len(loaded_val_data), img_res[0], img_res[1], img_res[2], 1))\n",
        "test_image_data = loaded_test_data.reshape((len(loaded_test_data), img_res[0], img_res[1], img_res[2], 1))\n",
        "print(train_image_data.shape, val_image_data.shape,test_image_data.shape)\n",
        "\n",
        "le = LabelEncoder()\n",
        "transformed_train_labels = le.fit_transform(loaded_train_labels)\n",
        "transformed_val_labels = le.transform(loaded_val_labels)\n",
        "transformed_test_labels = le.transform(loaded_test_labels)\n",
        "\n",
        "print(f'Shape of training data: {train_image_data.shape}, Shape of training labels: {transformed_train_labels.shape}')\n",
        "print(f'Shape of validation data: {val_image_data.shape}, Shape of training labels: {transformed_val_labels.shape}')\n",
        "print(f'Shape of training data: {test_image_data.shape}, Shape of training labels: {transformed_test_labels.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "802799b4",
      "metadata": {
        "id": "802799b4"
      },
      "source": [
        "Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab8711c4",
      "metadata": {
        "id": "ab8711c4"
      },
      "outputs": [],
      "source": [
        "# define an instance of the early_stopping class\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "monitor='val_accuracy',\n",
        "verbose=1,\n",
        "patience=5,\n",
        "mode='max',\n",
        "restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11c92e2a",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "11c92e2a",
        "outputId": "0f104c97-875a-4cf4-f11f-7e43fb45fed0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">27,680</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">55,360</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │       <span style=\"color: #00af00; text-decoration-color: #00af00\">110,656</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling3d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">221,312</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">442,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65536</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">33,554,944</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,052</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv_1 (\u001b[38;5;33mConv3D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m) │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_2 (\u001b[38;5;33mConv3D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m) │        \u001b[38;5;34m27,680\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling3d (\u001b[38;5;33mMaxPooling3D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_3 (\u001b[38;5;33mConv3D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m) │        \u001b[38;5;34m55,360\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_4 (\u001b[38;5;33mConv3D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m) │       \u001b[38;5;34m110,656\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling3d_1 (\u001b[38;5;33mMaxPooling3D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_5 (\u001b[38;5;33mConv3D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │       \u001b[38;5;34m221,312\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv_6 (\u001b[38;5;33mConv3D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │       \u001b[38;5;34m442,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65536\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │    \u001b[38;5;34m33,554,944\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │         \u001b[38;5;34m2,052\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,415,396</span> (131.28 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,415,396\u001b[0m (131.28 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,415,396</span> (131.28 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,415,396\u001b[0m (131.28 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m520s\u001b[0m 17s/step - accuracy: 0.3730 - loss: 1.6158 - val_accuracy: 0.4242 - val_loss: 1.2659\n",
            "Epoch 2/20\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m551s\u001b[0m 16s/step - accuracy: 0.4399 - loss: 1.2524 - val_accuracy: 0.6667 - val_loss: 1.1286\n",
            "Epoch 3/20\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m515s\u001b[0m 17s/step - accuracy: 0.6355 - loss: 1.0126 - val_accuracy: 0.7273 - val_loss: 0.7239\n",
            "Epoch 4/20\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m541s\u001b[0m 16s/step - accuracy: 0.7173 - loss: 0.7132 - val_accuracy: 0.6970 - val_loss: 0.6224\n",
            "Epoch 5/20\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 16s/step - accuracy: 0.7705 - loss: 0.5291 - val_accuracy: 0.6970 - val_loss: 0.7310\n",
            "Epoch 6/20\n",
            "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 16s/step - accuracy: 0.8276 - loss: 0.4363 - val_accuracy: 0.8182 - val_loss: 0.6060\n",
            "Epoch 7/20\n",
            "\u001b[1m28/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m48s\u001b[0m 16s/step - accuracy: 0.9068 - loss: 0.2445 "
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "from keras import regularizers\n",
        "\n",
        "#Establish number of classes we need to predict\n",
        "classes = 4\n",
        "#Hyperparameters\n",
        "lr = 0.001\n",
        "\n",
        "# initialize model\n",
        "model_tf = tf.keras.Sequential()\n",
        "\n",
        "# add convolutional layer\n",
        "model_tf.add(layers.Conv3D(\n",
        "  filters=32,\n",
        "  kernel_size=(3, 3, 3),\n",
        "  strides=(1, 1, 1),\n",
        "  padding='same',\n",
        "  data_format='channels_last',\n",
        "  name='conv_1',\n",
        "  activation='relu'\n",
        "))\n",
        "\n",
        "# add convolutional layer\n",
        "model_tf.add(layers.Conv3D(\n",
        "  filters=32,\n",
        "  kernel_size=(3, 3, 3),\n",
        "  strides=(1, 1, 1),\n",
        "  padding='same',\n",
        "  data_format='channels_last',\n",
        "  name='conv_2',\n",
        "  activation='relu'\n",
        "))\n",
        "\n",
        "# add max pooling layer\n",
        "model_tf.add(layers.MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# add dropout layer\n",
        "model_tf.add(layers.Dropout(rate=0.4))\n",
        "\n",
        "# add convolutional layer\n",
        "model_tf.add(layers.Conv3D(\n",
        "  filters=64,\n",
        "  kernel_size=(3, 3, 3),\n",
        "  strides=(1, 1, 1),\n",
        "  padding='same',\n",
        "  data_format='channels_last',\n",
        "  name='conv_3',\n",
        "  activation='relu'\n",
        "))\n",
        "\n",
        "# add convolutional layer\n",
        "model_tf.add(layers.Conv3D(\n",
        "  filters=64,\n",
        "  kernel_size=(3, 3, 3),\n",
        "  strides=(1, 1, 1),\n",
        "  padding='same',\n",
        "  data_format='channels_last',\n",
        "  name='conv_4',\n",
        "  activation='relu'\n",
        "))\n",
        "\n",
        "# add max pooling layer\n",
        "model_tf.add(layers.MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# add dropout layer\n",
        "model_tf.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "# add convolutional layer\n",
        "model_tf.add(layers.Conv3D(\n",
        "  filters=128,\n",
        "  kernel_size=(3, 3, 3),\n",
        "  strides=(1, 1, 1),\n",
        "  padding='same',\n",
        "  data_format='channels_last',\n",
        "  name='conv_5',\n",
        "  activation='relu'\n",
        "))\n",
        "\n",
        "# add convolutional layer\n",
        "model_tf.add(layers.Conv3D(\n",
        "  filters=128,\n",
        "  kernel_size=(3, 3, 3),\n",
        "  strides=(1, 1, 1),\n",
        "  padding='same',\n",
        "  data_format='channels_last',\n",
        "  name='conv_6',\n",
        "  activation='relu'\n",
        "))\n",
        "\n",
        "# add a flattening layer\n",
        "model_tf.add(layers.Flatten())\n",
        "\n",
        "# add a Dense Layer\n",
        "model_tf.add(layers.Dense(units=512, activation='relu'))\n",
        "\n",
        "# add the classification layer\n",
        "model_tf.add(layers.Dense(units=classes, activation='softmax', kernel_regularizer=regularizers.l2(1e-4))) #4 classes (softmax for multi-class)\n",
        "\n",
        "# build and compile model\n",
        "model_tf.build(input_shape=(None, 32, 32, 32, 1))\n",
        "model_tf.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(), #Since we are using Sparse encodings (not One-Hot Dense....)\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# print model_tf summary\n",
        "model_tf.summary()\n",
        "\n",
        "# train model_tf on (X_train, y_train) data\n",
        "history = model_tf.fit(\n",
        "    train_image_data,\n",
        "    transformed_train_labels,\n",
        "    validation_data=(val_image_data, transformed_val_labels),\n",
        "    epochs=20,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# plot loss curves\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Epochs v Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "# evaluate the accuracy of model_tf on (X_train, y_train) and (X_val, y_val)\n",
        "train_loss, train_acc = model_tf.evaluate(train_image_data, transformed_train_labels, verbose=1)\n",
        "val_loss, val_acc = model_tf.evaluate(val_image_data, transformed_val_labels, verbose=1)\n",
        "\n",
        "print(f\"\\nTrain Accuracy: {train_acc:.2f}\")\n",
        "print(f\"Validation Accuracy: {val_acc:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TRCd2vtSRBYE",
      "metadata": {
        "id": "TRCd2vtSRBYE"
      },
      "source": [
        "Evaluating Model on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cwHSICjQRBCc",
      "metadata": {
        "id": "cwHSICjQRBCc"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model_tf.evaluate(test_image_data, transformed_test_labels, verbose=1)\n",
        "\n",
        "print(f\"\\nTest Accuracy: {test_acc:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BARb6MQ5Fgmw",
      "metadata": {
        "id": "BARb6MQ5Fgmw"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get predictions on the test set\n",
        "y_pred_probs = model_tf.predict(test_image_data)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(transformed_test_labels, y_pred)\n",
        "\n",
        "# Get class names from the label encoder\n",
        "class_names = le.classes_\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(transformed_test_labels, y_pred, target_names=class_names))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "medical_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}