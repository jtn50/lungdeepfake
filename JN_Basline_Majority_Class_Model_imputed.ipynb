{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dfbe9bf4",
      "metadata": {
        "id": "dfbe9bf4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "#ML Packages\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68a1fd6e",
      "metadata": {
        "id": "68a1fd6e"
      },
      "source": [
        "Load Processed Dataset & Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "007bec43",
      "metadata": {
        "id": "007bec43"
      },
      "outputs": [],
      "source": [
        "def load_scan(path2scan):\n",
        "    if (path2scan.split('.')[-1] == 'mhd') or (path2scan.split('.')[-1] == 'raw'):\n",
        "        return load_mhd(path2scan)\n",
        "    elif path2scan.split('.')[-1] == 'dcm':\n",
        "        return load_dicom(os.path.split(path2scan)[0]) #pass containing directory\n",
        "    elif os.path.isdir(path2scan) and any(f.endswith('.dcm') for f in os.listdir(path2scan)):\n",
        "        return load_dicom(path2scan)\n",
        "    else:\n",
        "        raise Exception('No valid scan [series] found in given file/directory')\n",
        "\n",
        "def load_mhd(path2scan):\n",
        "    itkimage = sitk.ReadImage(path2scan)\n",
        "    scan = sitk.GetArrayFromImage(itkimage)\n",
        "    spacing = np.flip(np.array(itkimage.GetSpacing()),axis=0)\n",
        "    orientation = np.transpose(np.array(itkimage.GetDirection()).reshape((3, 3)))\n",
        "    origin = np.flip(np.array(itkimage.GetOrigin()),axis=0)\n",
        "    return scan, spacing, orientation, origin, None\n",
        "\n",
        "def load_dicom(path2scan_dir):\n",
        "    dicom_folder = path2scan_dir\n",
        "    dcms = [f for f in os.listdir(dicom_folder) if f.endswith('.dcm')]\n",
        "    if not dcms:\n",
        "        raise Exception(f\"No DICOM files found in {dicom_folder}\")\n",
        "\n",
        "    first_slice_data = pydicom.dcmread(os.path.join(path2scan_dir, dcms[0]))\n",
        "    first_slice = first_slice_data.pixel_array\n",
        "    orientation = np.transpose(first_slice_data.ImageOrientationPatient)\n",
        "    spacing_xy = np.array(first_slice_data.PixelSpacing, dtype=float)\n",
        "    spacing_z = float(first_slice_data.SliceThickness)\n",
        "    spacing = np.array([spacing_z, spacing_xy[1], spacing_xy[0]])\n",
        "\n",
        "    scan = np.zeros((len(dcms), first_slice.shape[0], first_slice.shape[1]))\n",
        "    raw_slices = []\n",
        "    indexes = []\n",
        "\n",
        "    for dcm in dcms:\n",
        "        slice_data = pydicom.dcmread(os.path.join(dicom_folder, dcm))\n",
        "        slice_data.filename = dcm\n",
        "        raw_slices.append(slice_data)\n",
        "        indexes.append(float(slice_data.ImagePositionPatient[2]))\n",
        "\n",
        "    indexes = np.array(indexes, dtype=float)\n",
        "    raw_slices = [x for _, x in sorted(zip(indexes, raw_slices))]\n",
        "\n",
        "    try:\n",
        "        origin = np.array(raw_slices[0][0x00200032].value)\n",
        "        origin = np.array([origin[2], origin[1], origin[0]])\n",
        "    except:\n",
        "        origin = np.zeros(3)\n",
        "\n",
        "    for i, slice_obj in enumerate(raw_slices):\n",
        "        scan[i, :, :] = slice_obj.pixel_array\n",
        "\n",
        "    return scan, spacing, orientation, origin, raw_slices\n",
        "\n",
        "def scale_scan(scan, spacing, factor=1):\n",
        "    resize_factor = factor * spacing\n",
        "    new_real_shape = scan.shape * resize_factor\n",
        "    new_shape = np.round(new_real_shape)\n",
        "    real_resize_factor = new_shape / scan.shape\n",
        "    new_spacing = spacing / real_resize_factor\n",
        "    scan_resized = scipy.ndimage.zoom(scan, real_resize_factor, mode='nearest')\n",
        "    return scan_resized, resize_factor\n",
        "\n",
        "def get_scaled_shape(shape, spacing):\n",
        "    new_real_shape = shape * spacing\n",
        "    return np.round(new_real_shape).astype(int)\n",
        "\n",
        "def world2vox(world_coord, spacing, orientation, origin):\n",
        "    world_coord = np.dot(np.linalg.inv(np.dot(orientation, np.diag(spacing))), world_coord - origin)\n",
        "    if orientation[0, 0] < 0:\n",
        "        vox_coord = (np.array([world_coord[0], world_coord[2], world_coord[1]])).astype(int)\n",
        "    else:\n",
        "        vox_coord = (np.array([world_coord[0], world_coord[1], world_coord[2]])).astype(int)\n",
        "    return vox_coord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5c313462",
      "metadata": {
        "id": "5c313462"
      },
      "outputs": [],
      "source": [
        "class LabeledExtractor:\n",
        "    \"\"\"\n",
        "    Adapted extractor for labeled medical imaging dataset to meet new pydicom requirements\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 labels_csv_path,\n",
        "                 dicom_base_dir,\n",
        "                 dst_path,\n",
        "                 norm_save_dir,\n",
        "                 cube_shape=(32, 32, 32),\n",
        "                 parallelize=False,\n",
        "                 coordSystem='vox',\n",
        "                 include_types=['FB', 'FM', 'TB', 'TM'],\n",
        "                 augment=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            labels_csv_path: Path to labels file\n",
        "            dicom_base_dir: Base directory containing DICOM scans organized by uuid\n",
        "            dst_path: Where to save the processed dataset\n",
        "            norm_save_dir: Where to save normalization parameters\n",
        "            cube_shape: Size of extracted 3D cubes\n",
        "            parallelize: Whether to use multiprocessing\n",
        "            coordSystem: 'vox' for voxel coordinates\n",
        "            include_types: Which finding types to include\n",
        "            augment: Whether to perform data augmentation\n",
        "        \"\"\"\n",
        "        self.labels_csv_path = labels_csv_path\n",
        "        self.dicom_base_dir = dicom_base_dir\n",
        "        self.dst_path = dst_path\n",
        "        self.norm_save_dir = norm_save_dir\n",
        "        self.cube_shape = cube_shape\n",
        "        self.parallelize = parallelize\n",
        "        self.coordSystem = coordSystem\n",
        "        self.include_types = include_types\n",
        "        self.augment = augment\n",
        "\n",
        "        # Load and filter the labels\n",
        "        self.labels_df = pd.read_csv(labels_csv_path)\n",
        "        self.labels_df = self.labels_df[self.labels_df['type'].isin(include_types)]\n",
        "\n",
        "        # # Filter out TB entries with 0,0,0 coordinates (no specific location)\n",
        "        # self.labels_df = self.labels_df[~((self.labels_df['type'] == 'TB') &\n",
        "        #                                 (self.labels_df['x'] == 0) &\n",
        "        #                                 (self.labels_df['y'] == 0) &\n",
        "        #                                 (self.labels_df['slice'] == 0))]\n",
        "\n",
        "        print(f\"Loaded {len(self.labels_df)} labeled samples\")\n",
        "        print(f\"Types distribution:\\n{self.labels_df['type'].value_counts()}\")\n",
        "\n",
        "    def extract(self, plot=True):\n",
        "        \"\"\"Extract and process all labeled samples\"\"\"\n",
        "        print(\"Preparing extraction jobs...\")\n",
        "\n",
        "        jobs = []\n",
        "        labels = []\n",
        "\n",
        "        for idx, row in self.labels_df.iterrows():\n",
        "            # Build path to DICOM scan\n",
        "            scan_path = os.path.join(self.dicom_base_dir, str(row['uuid']))\n",
        "\n",
        "            # Coordinate in z,y,x format (slice, y, x)\n",
        "            coord = np.array([row['slice'], row['y'], row['x']])\n",
        "\n",
        "            # Job format: [scan_path, coord, cube_shape, coordSystem, label_type]\n",
        "            jobs.append([scan_path, coord, self.cube_shape, self.coordSystem, row['type']])\n",
        "            labels.append(row['type'])\n",
        "\n",
        "        print(f\"Extracting {len(jobs)} samples...\")\n",
        "\n",
        "        if self.parallelize:\n",
        "            num_cores = int(np.ceil(min(np.ceil(multiprocessing.cpu_count() * 0.75), len(jobs))))\n",
        "            results = Parallel(n_jobs=num_cores)(delayed(self._process_job)(job) for job in jobs)\n",
        "        else:\n",
        "            results = []\n",
        "            for i, job in enumerate(jobs):\n",
        "                try:\n",
        "                    result = self._process_job(job)\n",
        "                    results.append(result)\n",
        "                    if i % 10 == 0:\n",
        "                        print(f\"Processed {i+1}/{len(jobs)} samples\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to process sample {job[0]}: {e}\")\n",
        "                    results.append(None)\n",
        "\n",
        "        # Collect successful extractions\n",
        "        instances = []\n",
        "        instance_labels = []\n",
        "\n",
        "        for i, result in enumerate(results):\n",
        "            if result is not None:\n",
        "                cubes, label_type = result\n",
        "                instances.extend(cubes)\n",
        "                instance_labels.extend([label_type] * len(cubes))\n",
        "\n",
        "        instances = np.array(instances)\n",
        "        instance_labels = np.array(instance_labels)\n",
        "\n",
        "        if len(instances) == 0:\n",
        "            print(\"ERROR: No instances were successfully extracted!\")\n",
        "            print(\"This could be due to:\")\n",
        "            print(\"1. DICOM loading issues\")\n",
        "            print(\"2. Coordinate problems\")\n",
        "            print(\"3. Missing DICOM files\")\n",
        "            return None, None, None\n",
        "\n",
        "        print(f\"Successfully extracted {len(instances)} instances\")\n",
        "\n",
        "        # Preprocessing\n",
        "        print(\"Equalizing the data...\")\n",
        "        eq = histEq(instances)\n",
        "        instances = eq.equalize(instances)\n",
        "        os.makedirs(self.norm_save_dir, exist_ok=True)\n",
        "        eq.save(path=os.path.join(self.norm_save_dir, 'equalization.pkl'))\n",
        "\n",
        "        print(\"Normalizing the data...\")\n",
        "        min_v = np.min(instances)\n",
        "        max_v = np.max(instances)\n",
        "        mean_v = np.mean(instances)\n",
        "        norm_data = np.array([mean_v, min_v, max_v])\n",
        "        instances = (instances - mean_v) / (max_v - min_v)\n",
        "        np.save(os.path.join(self.norm_save_dir, 'normalization.npy'), norm_data)\n",
        "\n",
        "        if plot:\n",
        "            self._plot_samples(instances, instance_labels)\n",
        "\n",
        "        # Save dataset and labels\n",
        "        print(\"Saving the dataset...\")\n",
        "        np.save(self.dst_path, instances)\n",
        "        np.save(self.dst_path.replace('.npy', '_labels.npy'), instance_labels)\n",
        "\n",
        "        # Create label mapping\n",
        "        label_mapping = {label: idx for idx, label in enumerate(np.unique(instance_labels))}\n",
        "        print(f\"Label mapping: {label_mapping}\")\n",
        "\n",
        "        return instances, instance_labels, label_mapping\n",
        "\n",
        "    def _process_job(self, args):\n",
        "        \"\"\"Process single extraction job\"\"\"\n",
        "        scan_path, coord, cube_shape, coordSystem, label_type = args\n",
        "\n",
        "        try:\n",
        "            # Load scan\n",
        "            scan, spacing, orientation, origin, raw_slices = load_scan(scan_path)\n",
        "\n",
        "            # Convert coordinates if needed\n",
        "            if coordSystem == 'world':\n",
        "                coord = world2vox(coord, spacing, orientation, origin)\n",
        "\n",
        "            # Extract the base cube with padding\n",
        "            init_cube_shape = get_scaled_shape(np.array(cube_shape) + 8, 1/spacing)\n",
        "            clean_cube_unscaled = cutCube(scan, coord, init_cube_shape, padd=-1000)\n",
        "\n",
        "            # Scale the cube\n",
        "            scaled_cube, resize_factor = scale_scan(clean_cube_unscaled, spacing)\n",
        "\n",
        "            # Data augmentation\n",
        "            if self.augment:\n",
        "                augmented_cubes = self._augment_instance(scaled_cube)\n",
        "            else:\n",
        "                augmented_cubes = [scaled_cube]\n",
        "\n",
        "            # Trim to final shape\n",
        "            final_cubes = []\n",
        "            for cube in augmented_cubes:\n",
        "                center = np.array(cube.shape) // 2\n",
        "                final_cube = cutCube(cube, center, cube_shape, padd=-1000)\n",
        "                if final_cube.shape == tuple(cube_shape):  # Only keep properly shaped cubes\n",
        "                    final_cubes.append(final_cube)\n",
        "\n",
        "            return final_cubes, label_type\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {scan_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _augment_instance(self, x0):\n",
        "        \"\"\"Data augmentation similar to original code but simplified\"\"\"\n",
        "        augmented = [x0]  # Original\n",
        "\n",
        "        # Flips\n",
        "        augmented.append(np.flip(x0, 1))  # x flip\n",
        "        augmented.append(np.flip(x0, 2))  # y flip\n",
        "\n",
        "        # Small shifts\n",
        "        augmented.append(scipy.ndimage.shift(x0, (0, 2, 2), mode='constant'))\n",
        "        augmented.append(scipy.ndimage.shift(x0, (0, -2, 2), mode='constant'))\n",
        "\n",
        "        # Small rotations\n",
        "        for angle in [15, 30, 45, 90, 180]:\n",
        "            rotated = rotate(x0, angle, axes=(1, 2), mode='reflect', reshape=False)\n",
        "            augmented.append(rotated)\n",
        "\n",
        "        # Filter out invalid shapes\n",
        "        valid_cubes = []\n",
        "        for cube in augmented:\n",
        "            if cube.shape[0] > 0 and cube.shape[1] > 0 and cube.shape[2] > 0:\n",
        "                valid_cubes.append(cube)\n",
        "\n",
        "        return valid_cubes\n",
        "\n",
        "    def _plot_samples(self, instances, labels):\n",
        "        \"\"\"Plot sample instances with their labels\"\"\"\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # Select random samples from each class\n",
        "        unique_labels = np.unique(labels)\n",
        "        samples_per_class = min(5, len(instances) // len(unique_labels))\n",
        "\n",
        "        fig, axes = plt.subplots(len(unique_labels), samples_per_class,\n",
        "                                figsize=(samples_per_class * 3, len(unique_labels) * 3))\n",
        "\n",
        "        if len(unique_labels) == 1:\n",
        "            axes = axes.reshape(1, -1)\n",
        "\n",
        "        for i, label in enumerate(unique_labels):\n",
        "            label_indices = np.where(labels == label)[0]\n",
        "            selected_indices = np.random.choice(label_indices,\n",
        "                                              min(samples_per_class, len(label_indices)),\n",
        "                                              replace=False)\n",
        "\n",
        "            for j, idx in enumerate(selected_indices):\n",
        "                middle_slice = instances[idx].shape[0] // 2\n",
        "                axes[i, j].imshow(instances[idx][middle_slice, :, :], cmap='bone')\n",
        "                axes[i, j].set_title(f'{label}')\n",
        "                axes[i, j].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.suptitle('Sample Extracted Instances by Type')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a4bbb818",
      "metadata": {
        "id": "a4bbb818"
      },
      "outputs": [],
      "source": [
        "class LabeledDataLoader:\n",
        "    \"\"\"\n",
        "    Data loader for labeled dataset - modified for classification\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset_path, labels_path, normdata_path, img_res=(32, 32, 32)):\n",
        "        self.normdata_path = normdata_path\n",
        "        self.img_res = img_res\n",
        "\n",
        "        print(\"Loading preprocessed dataset...\")\n",
        "        self.data = np.load(dataset_path)\n",
        "        self.labels = np.load(labels_path)\n",
        "\n",
        "        # Create label mapping\n",
        "        self.unique_labels = np.unique(self.labels)\n",
        "        self.label_to_idx = {label: idx for idx, label in enumerate(self.unique_labels)}\n",
        "        self.idx_to_label = {idx: label for label, idx in self.label_to_idx.items()}\n",
        "\n",
        "        # Convert labels to indices\n",
        "        self.label_indices = np.array([self.label_to_idx[label] for label in self.labels])\n",
        "\n",
        "        # Format for neural network\n",
        "        self.data = self.data.reshape((len(self.data), self.img_res[0], self.img_res[1], self.img_res[2], 1))\n",
        "\n",
        "        print(f\"Loaded {len(self.data)} samples\")\n",
        "        print(f\"Label distribution: {dict(zip(*np.unique(self.labels, return_counts=True)))}\")\n",
        "\n",
        "    def load_batch(self, batch_size=32, shuffle=True):\n",
        "        \"\"\"Load batches for training\"\"\"\n",
        "        if shuffle:\n",
        "            indices = np.random.permutation(len(self.data))\n",
        "        else:\n",
        "            indices = np.arange(len(self.data))\n",
        "\n",
        "        n_batches = len(self.data) // batch_size\n",
        "\n",
        "        for i in range(n_batches):\n",
        "            batch_indices = indices[i * batch_size:(i + 1) * batch_size]\n",
        "            batch_data = self.data[batch_indices]\n",
        "            batch_labels = self.label_indices[batch_indices]\n",
        "\n",
        "            yield batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "626f555c",
      "metadata": {
        "id": "626f555c"
      },
      "outputs": [],
      "source": [
        "def create_eda_dataframe(labels_csv_path, processed_data_path=None, processed_labels_path=None):\n",
        "    \"\"\"\n",
        "    Create dataframe for EDA\n",
        "    \"\"\"\n",
        "    # Load original labels CSV\n",
        "    print(\"Loading original labels...\")\n",
        "    df_filtered = pd.read_csv(labels_csv_path)\n",
        "\n",
        "    # # Filter out TB entries with 0,0,0 coordinates\n",
        "    # df_filtered = df_original[df_original['type'].isin(['FB', 'FM', 'TM'])]\n",
        "    # df_filtered = df_filtered[~((df_filtered['type'] == 'TB') &\n",
        "    #                           (df_filtered['x'] == 0) &\n",
        "    #                           (df_filtered['y'] == 0) &\n",
        "    #                           (df_filtered['slice'] == 0))]\n",
        "\n",
        "    #print(f\"Original labels: {len(df_original)}\")\n",
        "    print(f\"After filtering: {len(df_filtered)}\")\n",
        "\n",
        "    # Add encoded labels for visualization\n",
        "    le = LabelEncoder()\n",
        "    df_filtered['type_encoded'] = le.fit_transform(df_filtered['type'])\n",
        "\n",
        "    # add augmentation info\n",
        "    if processed_data_path and processed_labels_path:\n",
        "        try:\n",
        "            processed_labels = np.load(processed_labels_path)\n",
        "            print(f\"Processed dataset size: {len(processed_labels)}\")\n",
        "\n",
        "            # Calculate augmentation factor\n",
        "            augmentation_factor = len(processed_labels) / len(df_filtered)\n",
        "            df_filtered['augmentation_factor'] = augmentation_factor\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(\"Processed data not found, using original data only\")\n",
        "            df_filtered['augmentation_factor'] = 1\n",
        "\n",
        "    return df_filtered, le\n",
        "\n",
        "def plot_class_distribution(df):\n",
        "    \"\"\"Create class distribution bar plot\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Count by class\n",
        "    class_counts = df['type'].value_counts()\n",
        "    classes = class_counts.index\n",
        "    counts = class_counts.values\n",
        "\n",
        "    # Color scheme\n",
        "    colors = {'FB': '#FF6B6B', 'FM': '#4ECDC4', 'TM': '#45B7D1'}\n",
        "    plot_colors = [colors[cls] for cls in classes]\n",
        "\n",
        "    # Create bar plot\n",
        "    bars = plt.bar(classes, counts, color=plot_colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "    # Styling\n",
        "    plt.title('Distribution of Tamper Types in Dataset', fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.xlabel('Tamper Classification', fontsize=12, fontweight='bold')\n",
        "    plt.ylabel('Number of Instances', fontsize=12, fontweight='bold')\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, count in zip(bars, counts):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "                str(count), ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
        "\n",
        "    # Add percentage labels\n",
        "    total = sum(counts)\n",
        "    for bar, count in zip(bars, counts):\n",
        "        percentage = (count/total) * 100\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()/2,\n",
        "                f'{percentage:.1f}%', ha='center', va='center',\n",
        "                fontweight='bold', fontsize=10, color='white')\n",
        "\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print statistics\n",
        "    print(\"\\\\n=== Class Distribution Statistics ===\")\n",
        "    for cls in classes:\n",
        "        count = class_counts[cls]\n",
        "        pct = (count/total) * 100\n",
        "        print(f\"{cls}: {count} instances ({pct:.1f}%)\")\n",
        "\n",
        "def plot_spatial_distribution(df):\n",
        "    \"\"\"Create 3D spatial distribution visualization\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    # Color mapping for tamper types\n",
        "    colors = {'FB': 0, 'FM': 1, 'TM': 2}\n",
        "    df['color_code'] = df['type'].map(colors)\n",
        "\n",
        "    # 1. X-Y plane distribution (axial view)\n",
        "    scatter1 = axes[0,0].scatter(df['x'], df['y'], c=df['color_code'],\n",
        "                                cmap='viridis', alpha=0.7, s=50, edgecolors='black')\n",
        "    axes[0,0].set_xlabel('X Coordinate (Left-Right)', fontsize=10, fontweight='bold')\n",
        "    axes[0,0].set_ylabel('Y Coordinate (Anterior-Posterior)', fontsize=10, fontweight='bold')\n",
        "    axes[0,0].set_title('Tamper Locations in X-Y Plane (Axial View)', fontsize=12, fontweight='bold')\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. X-Z plane distribution (coronal view)\n",
        "    scatter2 = axes[0,1].scatter(df['x'], df['slice'], c=df['color_code'],\n",
        "                                cmap='viridis', alpha=0.7, s=50, edgecolors='black')\n",
        "    axes[0,1].set_xlabel('X Coordinate (Left-Right)', fontsize=10, fontweight='bold')\n",
        "    axes[0,1].set_ylabel('Z Coordinate (Slice Number)', fontsize=10, fontweight='bold')\n",
        "    axes[0,1].set_title('Tamper Locations in X-Z Plane (Coronal View)', fontsize=12, fontweight='bold')\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Y-Z plane distribution (sagittal view)\n",
        "    scatter3 = axes[1,0].scatter(df['y'], df['slice'], c=df['color_code'],\n",
        "                                cmap='viridis', alpha=0.7, s=50, edgecolors='black')\n",
        "    axes[1,0].set_xlabel('Y Coordinate (Anterior-Posterior)', fontsize=10, fontweight='bold')\n",
        "    axes[1,0].set_ylabel('Z Coordinate (Slice Number)', fontsize=10, fontweight='bold')\n",
        "    axes[1,0].set_title('Tamper Locations in Y-Z Plane (Sagittal View)', fontsize=12, fontweight='bold')\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. 3D histogram\n",
        "    axes[1,1].hist2d(df['x'], df['y'], bins=20, cmap='Blues', alpha=0.8)\n",
        "    axes[1,1].set_xlabel('X Coordinate', fontsize=10, fontweight='bold')\n",
        "    axes[1,1].set_ylabel('Y Coordinate', fontsize=10, fontweight='bold')\n",
        "    axes[1,1].set_title('Spatial Density Heatmap', fontsize=12, fontweight='bold')\n",
        "\n",
        "    # Add colorbar for tamper types\n",
        "    cbar = plt.colorbar(scatter1, ax=axes[0,0], ticks=[0, 1, 2])\n",
        "    cbar.set_ticklabels(['FB', 'FM', 'TM'])\n",
        "    cbar.set_label('Tamper Type', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print spatial statistics\n",
        "    print(\"\\\\n=== Spatial Distribution Statistics ===\")\n",
        "    for coord in ['x', 'y', 'slice']:\n",
        "        print(f\"{coord.upper()} coordinate:\")\n",
        "        print(f\"  Range: [{df[coord].min()}, {df[coord].max()}]\")\n",
        "        print(f\"  Mean: {df[coord].mean():.1f} ± {df[coord].std():.1f}\")\n",
        "        print(f\"  Median: {df[coord].median():.1f}\")\n",
        "\n",
        "def plot_correlation_analysis(df):\n",
        "    \"\"\"Create correlation analysis visualization\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # 1. Correlation heatmap\n",
        "    coords = ['x', 'y', 'slice']\n",
        "    corr_matrix = df[coords].corr()\n",
        "\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "                square=True, ax=axes[0], cbar_kws={'label': 'Correlation Coefficient'})\n",
        "    axes[0].set_title('Spatial Coordinate Correlations', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # 2. Pairwise relationships\n",
        "    # Create subplot for pairwise scatter\n",
        "    axes[1].scatter(df['x'], df['y'], alpha=0.6, s=30, label='X vs Y')\n",
        "    axes[1].set_xlabel('X Coordinate', fontweight='bold')\n",
        "    axes[1].set_ylabel('Y Coordinate', fontweight='bold')\n",
        "    axes[1].set_title('X-Y Coordinate Relationship', fontsize=14, fontweight='bold')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Add trend line\n",
        "    z = np.polyfit(df['x'], df['y'], 1)\n",
        "    p = np.poly1d(z)\n",
        "    axes[1].plot(df['x'], p(df['x']), \"r--\", alpha=0.8,\n",
        "                label=f'Trend (r={corr_matrix.loc[\"x\", \"y\"]:.3f})')\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print correlation insights\n",
        "    print(\"\\\\n=== Correlation Analysis ===\")\n",
        "    print(\"Correlation coefficients:\")\n",
        "    for i in range(len(coords)):\n",
        "        for j in range(i+1, len(coords)):\n",
        "            corr_val = corr_matrix.iloc[i, j]\n",
        "            coord1, coord2 = coords[i], coords[j]\n",
        "            strength = \"weak\" if abs(corr_val) < 0.3 else \"moderate\" if abs(corr_val) < 0.7 else \"strong\"\n",
        "            print(f\"  {coord1.upper()}-{coord2.upper()}: {corr_val:.3f} ({strength})\")\n",
        "\n",
        "def plot_class_characteristics(df):\n",
        "    \"\"\"Analyze characteristics by tamper type\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Box plots for each coordinate by class\n",
        "    coords = ['x', 'y', 'slice']\n",
        "    for i, coord in enumerate(coords):\n",
        "        if i < 3:  # We have 3 coordinates but 4 subplots\n",
        "            row, col = divmod(i, 2)\n",
        "            data_by_class = [df[df['type'] == cls][coord].values for cls in ['FB', 'FM', 'TM']]\n",
        "\n",
        "            bp = axes[row, col].boxplot(data_by_class, labels=['FB', 'FM', 'TM'],\n",
        "                                       patch_artist=True, notch=True)\n",
        "\n",
        "            # Color the boxes\n",
        "            colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
        "            for patch, color in zip(bp['boxes'], colors):\n",
        "                patch.set_facecolor(color)\n",
        "                patch.set_alpha(0.7)\n",
        "\n",
        "            axes[row, col].set_title(f'{coord.upper()} Coordinate by Tamper Type',\n",
        "                                   fontweight='bold', fontsize=12)\n",
        "            axes[row, col].set_ylabel(f'{coord.upper()} Value', fontweight='bold')\n",
        "            axes[row, col].grid(True, alpha=0.3)\n",
        "\n",
        "    # Patient distribution (UUID analysis)\n",
        "    uuid_counts = df['uuid'].value_counts()\n",
        "    axes[1, 1].hist(uuid_counts.values, bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    axes[1, 1].set_title('Tamper Instances per Patient', fontweight='bold', fontsize=12)\n",
        "    axes[1, 1].set_xlabel('Number of Tamper Sites per Patient', fontweight='bold')\n",
        "    axes[1, 1].set_ylabel('Number of Patients', fontweight='bold')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print class statistics\n",
        "    print(\"\\\\n=== Class-Specific Statistics ===\")\n",
        "    for tamper_type in ['FB', 'FM', 'TB', 'TM']:\n",
        "        subset = df[df['type'] == tamper_type]\n",
        "        print(f\"\\\\n{tamper_type} ({len(subset)} instances):\")\n",
        "        print(f\"  Unique patients: {subset['uuid'].nunique()}\")\n",
        "        print(f\"  Avg instances per patient: {len(subset)/subset['uuid'].nunique():.2f}\")\n",
        "        for coord in ['x', 'y', 'slice']:\n",
        "            print(f\"  {coord.upper()}: {subset[coord].mean():.1f} ± {subset[coord].std():.1f}\")\n",
        "\n",
        "def create_augmentation_visualization(original_data_path, augmented_data_path, labels_path):\n",
        "    \"\"\"Show before/after augmentation if data available\"\"\"\n",
        "    try:\n",
        "        original_data = np.load(original_data_path) if original_data_path else None\n",
        "        augmented_data = np.load(augmented_data_path)\n",
        "        labels = np.load(labels_path)\n",
        "\n",
        "        # Sample one from each class\n",
        "        fig, axes = plt.subplots(3, 6, figsize=(18, 9))\n",
        "\n",
        "        unique_labels = np.unique(labels)\n",
        "        for class_idx, label in enumerate(unique_labels):\n",
        "            class_indices = np.where(labels == label)[0]\n",
        "            sample_idx = class_indices[0]  # First sample of this class\n",
        "\n",
        "            # Show original\n",
        "            sample_cube = augmented_data[sample_idx]\n",
        "            middle_slice = sample_cube.shape[0] // 2\n",
        "\n",
        "            axes[class_idx, 0].imshow(sample_cube[middle_slice, :, :], cmap='bone')\n",
        "            axes[class_idx, 0].set_title(f'{label} - Original')\n",
        "            axes[class_idx, 0].axis('off')\n",
        "\n",
        "            # Show some augmented versions\n",
        "            augmentation_names = ['Example 2', 'Example 3', 'Example 4', 'Example 5', 'Example 6']\n",
        "            for aug_idx in range(1, 6):\n",
        "                if sample_idx + aug_idx < len(class_indices):\n",
        "                    aug_sample_idx = class_indices[sample_idx + aug_idx] if sample_idx + aug_idx < len(class_indices) else class_indices[-1]\n",
        "                    aug_cube = augmented_data[aug_sample_idx]\n",
        "                    axes[class_idx, aug_idx].imshow(aug_cube[middle_slice, :, :], cmap='bone')\n",
        "                    axes[class_idx, aug_idx].set_title(f'{label} - {augmentation_names[aug_idx-1]}')\n",
        "                else:\n",
        "                    axes[class_idx, aug_idx].text(0.5, 0.5, 'N/A', ha='center', va='center', transform=axes[class_idx, aug_idx].transAxes)\n",
        "                axes[class_idx, aug_idx].axis('off')\n",
        "\n",
        "        plt.suptitle('Sample Images by Tamper Type', fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"\\\\n=== Augmentation Statistics ===\")\n",
        "        print(f\"Final dataset size: {len(augmented_data)} cubes\")\n",
        "        print(f\"Augmentation factor: {len(augmented_data) / len(np.unique(labels))} per unique label\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Augmented data files not found. Skipping augmentation visualization.\")\n",
        "\n",
        "def run_complete_eda(labels_csv_path, processed_data_path=None, processed_labels_path=None):\n",
        "    \"\"\"\n",
        "    Run complete EDA pipeline\n",
        "    \"\"\"\n",
        "    print(\"=== MEDICAL IMAGE TAMPER DETECTION - EDA ===\\\\n\")\n",
        "\n",
        "    # Create dataframe\n",
        "    df, label_encoder = create_eda_dataframe(labels_csv_path, processed_data_path, processed_labels_path)\n",
        "\n",
        "    print(f\"\\\\nDataset loaded successfully!\")\n",
        "    print(f\"Total instances: {len(df)}\")\n",
        "    print(f\"Unique patients: {df['uuid'].nunique()}\")\n",
        "    print(f\"Tamper types: {df['type'].unique()}\")\n",
        "\n",
        "    # Run all visualizations\n",
        "    print(\"\\\\n\" + \"=\"*50)\n",
        "    print(\"1. CLASS DISTRIBUTION ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "    plot_class_distribution(df)\n",
        "\n",
        "    print(\"\\\\n\" + \"=\"*50)\n",
        "    print(\"2. SPATIAL DISTRIBUTION ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "    plot_spatial_distribution(df)\n",
        "\n",
        "    print(\"\\\\n\" + \"=\"*50)\n",
        "    print(\"3. CORRELATION ANALYSIS\")\n",
        "    print(\"=\"*50)\n",
        "    plot_correlation_analysis(df)\n",
        "\n",
        "    print(\"\\\\n\" + \"=\"*50)\n",
        "    print(\"4. CLASS CHARACTERISTICS\")\n",
        "    print(\"=\"*50)\n",
        "    plot_class_characteristics(df)\n",
        "\n",
        "    # If processed data available, show augmentation examples\n",
        "    if processed_data_path and processed_labels_path:\n",
        "        print(\"\\\\n\" + \"=\"*50)\n",
        "        print(\"5. AUGMENTATION EXAMPLES\")\n",
        "        print(\"=\"*50)\n",
        "        create_augmentation_visualization(None, processed_data_path, processed_labels_path)\n",
        "\n",
        "    return df, label_encoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "69e1c2e4",
      "metadata": {
        "id": "69e1c2e4"
      },
      "outputs": [],
      "source": [
        "#Original Labels\n",
        "#labels_csv = \"/mnt/c/Users/nhanj/Desktop/deepfakes+medical+image+tamper+detection/data/Tampered Scans/labels_exp1.csv\"\n",
        "labels_csv = \"labels_exp1_imputed.csv\"\n",
        "#Augmented & TB Class-Imputed Labels and Data\n",
        "processed_data = \"combined_tampered_scans_dataset.npy\"\n",
        "processed_labels = \"combined_tampered_scans_dataset_labels.npy\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c5096f07",
      "metadata": {
        "id": "c5096f07"
      },
      "outputs": [],
      "source": [
        "image_data = np.load(processed_data)\n",
        "image_labels = np.load(processed_labels) # Load labels here as well\n",
        "\n",
        "#Reshape to (# Samples, 32, 32, 32, 1) since we are dealing with 32x32x32 cubes we've extracted with 1 color channel (grayscale)\n",
        "img_res=(32, 32, 32)\n",
        "# Get the number of samples from the loaded data\n",
        "num_samples = image_data.shape[0]\n",
        "image_data = image_data.reshape((num_samples, img_res[0], img_res[1], img_res[2], 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95288cdf",
      "metadata": {
        "id": "95288cdf"
      },
      "source": [
        "Create Train, Validation, Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1a0821c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a0821c4",
        "outputId": "da7aa88d-27f8-47a3-d181-4dfb6caaffbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape (984, 32, 32, 32, 1)\n",
            "y_train shape (984,)\n",
            "X_val shape (328, 32, 32, 32, 1)\n",
            "y_val shape (328,)\n",
            "X_test shape (328, 32, 32, 32, 1)\n",
            "y_test shape (328,)\n"
          ]
        }
      ],
      "source": [
        "#Random Shuffle\n",
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "\n",
        "data_length = len(image_data)\n",
        "indices = np.arange(data_length)\n",
        "np.random.shuffle(indices)\n",
        "#Shuffled Datasets\n",
        "image_data = image_data[indices]\n",
        "image_labels = image_labels[indices]\n",
        "\n",
        "#Data Splits\n",
        "splits = (0.6, 0.2, 0.2) #60 train / 20 val / 20 test\n",
        "#Train Range\n",
        "train_end = int(splits[0] * data_length)\n",
        "#Val Range\n",
        "val_end = train_end + int(splits[1] * data_length)\n",
        "\n",
        "#Train Set\n",
        "X_train = image_data[:train_end]\n",
        "y_train = image_labels[:train_end]\n",
        "\n",
        "#Val Set\n",
        "X_val = image_data[train_end:val_end]\n",
        "y_val = image_labels[train_end:val_end]\n",
        "\n",
        "#Test Set\n",
        "X_test = image_data[val_end:]\n",
        "y_test = image_labels[val_end:]\n",
        "\n",
        "print(f\"X_train shape {X_train.shape}\")\n",
        "print(f\"y_train shape {y_train.shape}\")\n",
        "print(f\"X_val shape {X_val.shape}\")\n",
        "print(f\"y_val shape {y_val.shape}\")\n",
        "print(f\"X_test shape {X_test.shape}\")\n",
        "print(f\"y_test shape {y_test.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27fa3a2e",
      "metadata": {
        "id": "27fa3a2e"
      },
      "source": [
        "(Sparse / Integer) Encoding Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7901bfed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7901bfed",
        "outputId": "01273b0e-1120-41b3-cafc-cf3208168293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: FB\n",
            "After: 0\n"
          ]
        }
      ],
      "source": [
        "print(\"Before:\", y_train[0])\n",
        "\n",
        "le = LabelEncoder()\n",
        "#Fit on training\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "#Transform based on Training fit\n",
        "y_val_encoded = le.transform(y_val)\n",
        "#Transform based on Training fit\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "print(\"After:\", y_train_encoded[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "802799b4",
      "metadata": {
        "id": "802799b4"
      },
      "source": [
        "Modeling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Find majority class\n",
        "majority_class = np.bincount(y_train_encoded).argmax()\n",
        "print(y_train[majority_class])\n",
        "\n",
        "# Predict majority class for every example...\n",
        "y_baseline_pred = np.full_like(y_train_encoded, fill_value=majority_class)\n",
        "\n",
        "#  Evaluate baseline accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "baseline_acc = accuracy_score(y_train_encoded, y_baseline_pred)\n",
        "\n",
        "print(f\"Baseline Training Accuracy (majority class): {baseline_acc:.2f}\")\n",
        "\n",
        "# Predict majority class for every example...\n",
        "y_baseline_pred = np.full_like(y_val_encoded, fill_value=majority_class)\n",
        "\n",
        "# Evaluate baseline accuracy\n",
        "baseline_acc = accuracy_score(y_val_encoded, y_baseline_pred)\n",
        "\n",
        "print(f\"Baseline Validation Accuracy (majority class): {baseline_acc:.2f}\")\n",
        "\n",
        "# Predict majority class for every example...\n",
        "y_baseline_pred = np.full_like(y_test_encoded, fill_value=majority_class)\n",
        "\n",
        "# Evaluate baseline accuracy\n",
        "baseline_acc = accuracy_score(y_test_encoded, y_baseline_pred)\n",
        "\n",
        "print(f\"Baseline Test Accuracy (majority class): {baseline_acc:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EYYBBhYOdxo",
        "outputId": "6c13e86d-c9e6-4d4c-b06b-0b8aef80e7a4"
      },
      "id": "0EYYBBhYOdxo",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FB\n",
            "Baseline Training Accuracy (majority class): 0.44\n",
            "Baseline Validation Accuracy (majority class): 0.45\n",
            "Baseline Test Accuracy (majority class): 0.42\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "medical_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}